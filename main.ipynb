{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c145b5b1-188c-4275-a001-f9f94ed5fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d1d7f94-bcf0-4ddf-a5a0-1b4f1f453426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646a4ac1-7133-49cb-8b99-bee571b9d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_coco(df, output_path, category_name=\"object\", category_id=1):\n",
    "    \"\"\"\n",
    "    Converts a dataframe to a COCO JSON format.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with fields ['fname', 'height', 'width', 'bbox', 'num_balloons'].\n",
    "        output_path (str): Path to save the output COCO JSON file.\n",
    "        category_name (str): Name of the category (default: \"object\").\n",
    "        category_id (int): ID of the category (default: 1).\n",
    "\n",
    "    Returns:\n",
    "        None: Saves the COCO JSON file to the specified output path.\n",
    "    \"\"\"\n",
    "    coco_format = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": category_id, \"name\": category_name, \"supercategory\": \"none\"}]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1  # Unique ID for each annotation\n",
    "    for idx, row in df.iterrows():\n",
    "        image_id = idx + 1  # Unique ID for each image\n",
    "\n",
    "        # Add image information\n",
    "        coco_format[\"images\"].append({\n",
    "            \"id\": image_id,\n",
    "            \"file_name\": row[\"fname\"],\n",
    "            \"height\": row[\"height\"],\n",
    "            \"width\": row[\"width\"]\n",
    "        })\n",
    "\n",
    "        # Add annotations\n",
    "        for bbox in eval(row[\"bbox\"]):\n",
    "            # Convert bbox to COCO format: [x, y, width, height]\n",
    "            coco_bbox = [\n",
    "                bbox[\"xmin\"],\n",
    "                bbox[\"ymin\"],\n",
    "                bbox[\"xmax\"] - bbox[\"xmin\"],\n",
    "                bbox[\"ymax\"] - bbox[\"ymin\"]\n",
    "            ]\n",
    "\n",
    "            coco_format[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": coco_bbox,\n",
    "                \"area\": coco_bbox[2] * coco_bbox[3],  # width * height\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(coco_format, f, indent=4)\n",
    "\n",
    "    print(f\"COCO JSON saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbb1453-b23b-4c28-8413-2742e8c9ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO JSON saved to annotations.json\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('balloon-data.csv')\n",
    "# ann_name = 'annotations.json'\n",
    "# dataframe_to_coco(df, ann_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8891b8f7-ef4b-4e36-b271-c755b7b4c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(dataset, index, category_names=None):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes on an image from a COCO dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (torchvision.datasets.CocoDetection): The dataset object.\n",
    "        index (int): The index of the image in the dataset to visualize.\n",
    "        category_names (dict): Optional. A dictionary mapping category IDs to names for labeling.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the image with bounding boxes.\n",
    "    \"\"\"\n",
    "    # Load image and target from dataset\n",
    "    img, target = dataset[index]\n",
    "    \n",
    "    # Convert image to a format suitable for plotting\n",
    "    if isinstance(img, Image.Image):  # PIL Image\n",
    "        img_array = img\n",
    "    else:  # Tensor or other format\n",
    "        img_array = img.permute(1, 2, 0).numpy()  # Convert CHW -> HWC for plotting\n",
    "\n",
    "    # Create a plot\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(img_array)\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for i in range(len(target['boxes'])):\n",
    "\n",
    "\n",
    "        # Extract bounding box parameters: [x, y, width, height]\n",
    "        x, y, width, height = target['boxes'][i]\n",
    "        category_id = target['class_labels'][i]\n",
    "        # Add rectangle patch\n",
    "        rect = patches.Rectangle(\n",
    "            (x, y), width, height, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add category label if provided\n",
    "        if category_names and category_id in category_names:\n",
    "            label = category_names[category_id]\n",
    "            ax.text(\n",
    "                x,\n",
    "                y - 10,\n",
    "                label,\n",
    "                color=\"red\",\n",
    "                fontsize=12,\n",
    "                bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"none\"),\n",
    "            )\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b31b17-0633-4b09-8d11-d8b31ba371ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1028c08e-a450-4e19-bfb8-1e9b586bbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATION_FILE_NAME = \"annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c45e041-1b07-4bd0-ab4c-2619a126c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e73f2698-5ec2-41eb-93e6-8887f0acca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coco(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_directory_path: str, \n",
    "        processor, \n",
    "        transform = None\n",
    "    ):\n",
    "        super().__init__(image_directory_path, ANNOTATION_FILE_NAME, transform=transform)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, annotations = super().__getitem__(idx)        \n",
    "        image_id = self.ids[idx]\n",
    "        image = np.asarray(image)\n",
    "        if self.transform is not None:\n",
    "            df = pd.DataFrame(annotations)\n",
    "            t = self.transform(image=image, bboxes=df['bbox'].tolist(), class_labels=df['category_id'].tolist())\n",
    "            image = t['image']\n",
    "            df['bbox'] = t['bboxes']\n",
    "            df['area'] = df['bbox'].apply(lambda y: [x[2]*x[3] for x in y] if isinstance(y[0], list) else y[2]*y[3])\n",
    "            df['category_id'] = t['class_labels']\n",
    "            annotations = df.to_dict('records')\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.processor(images=image, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f92b8564-3013-421b-b173-62653c5919e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, torch\n",
    "import numpy as np\n",
    "def set_seed():\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6085ca-534d-4271-b131-2f35d4e2b2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Train size: 51\n",
      "Validation size: 11\n",
      "Test size: 12\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "full_dataset = Coco(image_directory_path='images/', processor=processor)\n",
    "\n",
    "\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "set_seed()\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b085ff24-dc45-4e96-8661-9cb6aba6ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from copy import copy\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "train_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),                     # Flip horizontally\n",
    "    A.RandomRotate90(p=0.3),                    # Rotate 90 degrees randomly  \n",
    "    # Color adjustments\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2,  # Random brightness and contrast\n",
    "                               contrast_limit=0.2, \n",
    "                               p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=10,          # Random hue, saturation, and value\n",
    "                         sat_shift_limit=15, \n",
    "                         val_shift_limit=10, \n",
    "                         p=0.5),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),     # Gaussian noise\n",
    "    A.MotionBlur(blur_limit=3, p=0.2),               # Simulate motion blur\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
    "\n",
    "train_dataset.dataset.transform = train_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2093fb65-9a24-40e0-bc45-8893ff4487eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # DETR authors employ various image sizes during training, making it not possible \n",
    "    # to directly batch together images. Hence they pad the images to the biggest \n",
    "    # resolution in a given batch, and create a corresponding binary pixel_mask \n",
    "    # which indicates which pixels are real/which are padding\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'],\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deda71d5-b510-4017-8580-ba66df0322b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178363c4-b456-44a1-bee6-46be180acd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48200125-4142-42b4-afb1-eb6c0bdab1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
